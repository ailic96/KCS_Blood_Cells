{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KCS_Blood_Cells.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c0bwqgR7aZep",
        "W7QWH67HaVbw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0NYNevqFHpZ"
      },
      "source": [
        "**Klasifikacija bijelih krvnih stanica**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbXzEIh2FREg"
      },
      "source": [
        "## Spajanje na google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4vHMzS3DcHH",
        "outputId": "cfb8f70e-37b2-4698-94d5-eba29af08735"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIMhqWwhZ-P"
      },
      "source": [
        "### Potrebni paketi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSi7Qn_Thcoz"
      },
      "source": [
        "import matplotlib.pyplot as plt     # plotting \r\n",
        "import numpy as np                  # Matrix image operations\r\n",
        "import cv2                          # Computer vision\r\n",
        "import glob                         # File path manipulation\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq5E4Rfb6rvN"
      },
      "source": [
        "### Putanje do mapa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJkJWHRQ6qbS"
      },
      "source": [
        "input_path_eosino = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL/'\r\n",
        "input_path_lympho = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE/'\r\n",
        "input_path_mono =   '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE/'\r\n",
        "input_path_neutro = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL/'\r\n",
        "\r\n",
        "output_path_eosino = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN_PROCESSED/EOSINOPHIL/'\r\n",
        "output_path_lympho = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN_PROCESSED/LYMPHOCYTE/'\r\n",
        "output_path_mono =   '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN_PROCESSED/MONOCYTE/'\r\n",
        "output_path_neutro = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/imagesTRAIN_PROCESSED/NEUTROPHIL/'"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qDsdMmFd8-"
      },
      "source": [
        "## Funkcije za pretprocesiranje slika"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP3cEefoFQyl"
      },
      "source": [
        "def create_directory(path):\r\n",
        "    \"\"\" Creates empty folder if folder doesn't  exist.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        path (string): Relative or absolute path for creating a folder\r\n",
        "    \"\"\"\r\n",
        "    %mkdir '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN_PROCESSED/'\r\n",
        "    \r\n",
        "    if not os.path.exists(path):\r\n",
        "      %mkdir '$path'\r\n",
        "      print('Creating folder structure at:', path)\r\n",
        "\r\n",
        "\r\n",
        "def remove_if_exists(path):\r\n",
        "    \"\"\" Removes all files on given path for fresh results\r\n",
        "    on every script run.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        path (string): Input path for file removal.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    for item in glob.glob(path):\r\n",
        "        if os.path.exists(path):\r\n",
        "            %rm -rf '$path'\r\n",
        "            print('Path ' + path + 'existed, so it was deleted.')\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "def find_cell(image):\r\n",
        "    \"\"\" Identifies cells in range of lower and upper blue RGB color spectrum.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image (numpy.ndarray): Image loaded through cv2 package.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        [numpy.ndarray]: Returns blue cell mask.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # RGB type blue color interval\r\n",
        "    lower_blue = np.array([90,80,160])\r\n",
        "    upper_blue = np.array([150,140,255])\r\n",
        "    \r\n",
        "    # Convert image to RGB\r\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
        "\r\n",
        "    # Create a mask in range of upper and lower blue\r\n",
        "    mask = cv2.inRange(image, lower_blue, upper_blue)\r\n",
        "\r\n",
        "    return mask\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def crop_image(path):\r\n",
        "    \"\"\" Crops image around masked part. Mask contains a region of interest \r\n",
        "    (A white cell).\r\n",
        "\r\n",
        "    Args:\r\n",
        "        path (string): String which contains image path.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        [numpy.ndarray]: Cropped image in a black frame.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    image = cv2.imread(path)\r\n",
        "    mask = find_cell(image)\r\n",
        "    #plt.imshow(mask, cmap='gray')\r\n",
        "    #plt.show()\r\n",
        "\r\n",
        "    # Find mask edges\r\n",
        "    indices = np.nonzero(mask)\r\n",
        "    size = len(set(zip(indices[0], indices[1])))\r\n",
        "    avgX, avgY = 0, 0\r\n",
        "    minX, minY = 10000, 10000\r\n",
        "    maxX, maxY = -1, -1\r\n",
        "\r\n",
        "    # Resize croped image dynamically\r\n",
        "    for (x, y) in set(zip(indices[0], indices[1])):\r\n",
        "        \r\n",
        "        minX, minY, maxX, maxY = min(minX, x), min(minY, y), max(maxX, x), max(maxY, y)\r\n",
        "        avgX += x\r\n",
        "        avgY += y\r\n",
        "\r\n",
        "    if size != 0:\r\n",
        "        avgX /= size\r\n",
        "    if size != 0:\r\n",
        "        avgY /= size\r\n",
        "\r\n",
        "    cropped = image[minX:maxX, minY:maxY]\r\n",
        "    \r\n",
        "    height, width = (333,333)\r\n",
        "\r\n",
        "    frame = np.zeros((400,400, 3), np.uint8)\r\n",
        "    x_offset = int((width - cropped.shape[1])/2)\r\n",
        "    y_offset = int((height - cropped.shape[0])/2)\r\n",
        "\r\n",
        "    frame[y_offset:y_offset+cropped.shape[0], x_offset:x_offset+cropped.shape[1]] = cropped\r\n",
        "\r\n",
        "    return frame\r\n",
        "\r\n",
        "\r\n",
        "def run(input_path, output_path):\r\n",
        "    \"\"\" Collects all functions, iterates through a folder structure, crops images from \r\n",
        "    input path and saves them to output file.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_path (string): Input images for cropping.\r\n",
        "        output_path (string): Output path for saving cropped images.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #remove_if_exists(output_path)\r\n",
        "    #create_directory(output_path)\r\n",
        "    \r\n",
        "\r\n",
        "    print('Processing', input_path)\r\n",
        "    for path in glob.iglob(input_path + '*.jpeg'):\r\n",
        "        \r\n",
        "        #print('Processing', path)\r\n",
        "        image = crop_image(path)\r\n",
        "        out = output_path + os.path.basename(path)\r\n",
        "        \r\n",
        "        # Skips generated empty black (numpy.zeros) masks, error prevention.\r\n",
        "        try:\r\n",
        "            cv2.imwrite (out, image)\r\n",
        "        except:\r\n",
        "            print('Couldn\\'t find a mask! Moving on.')\r\n",
        "            pass\r\n",
        "\r\n",
        "#run(input_path_eosino, output_path_eosino)\r\n",
        "#run(input_path_lympho, output_path_lympho)\r\n",
        "#run(input_path_mono, output_path_mono)\r\n",
        "#run(input_path_neutro, output_path_neutro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7BlsyYdFy4q"
      },
      "source": [
        "## Stvaranje modela"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfADRRIc62To",
        "outputId": "357aad20-d159-4b98-ea65-5b09bcbba570"
      },
      "source": [
        "\r\n",
        "%tensorflow_version 2.x\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Conv2D, Dropout, MaxPool2D, Flatten\r\n",
        "from keras.preprocessing import image\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "\r\n",
        "generator = image.ImageDataGenerator(\r\n",
        "        rescale = 1./255,\r\n",
        "        featurewise_center=False,           # set input mean to 0 over the dataset\r\n",
        "        samplewise_center=False,            # set each sample mean to 0\r\n",
        "        featurewise_std_normalization=False,# divide inputs by std of the dataset\r\n",
        "        samplewise_std_normalization=False, # divide each input by its std\r\n",
        "        zca_whitening=False,                # apply ZCA whi0tening\r\n",
        "        rotation_range=10,                  # randomly rotate images in the range (degrees, 0 to 180)\r\n",
        "        width_shift_range=0.1,              # randomly shift images horizontally (fraction of total width)\r\n",
        "        height_shift_range=0.1,             # randomly shift images vertically (fraction of total height)\r\n",
        "        horizontal_flip=True,               # randomly flip images\r\n",
        "        vertical_flip=False)\r\n",
        "\r\n",
        "dataset_default = generator.flow_from_directory(\r\n",
        "    shuffle = True,\r\n",
        "    batch_size = 32,\r\n",
        "    target_size = (80, 80),\r\n",
        "    directory = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN/'\r\n",
        ")\r\n",
        "\r\n",
        "dataset_processed = generator.flow_from_directory(\r\n",
        "    shuffle = True,\r\n",
        "    batch_size = 32,\r\n",
        "    target_size = (80, 80),\r\n",
        "    directory = '/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/TRAIN_PROCESSED/'\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def model():\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv2D(80, (3,3), strides = (1, 1), activation = 'relu'))\r\n",
        "    model.add(Conv2D(64, (3,3), strides = (1, 1), activation = 'relu', input_shape = (80, 80, 3)))\r\n",
        "    model.add(MaxPool2D(pool_size = (2,2)))\r\n",
        "    model.add(Conv2D(64, (3,3), strides = (1,1), activation = 'relu'))\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Flatten())\r\n",
        "\r\n",
        "    model.add(Dense(128, activation = 'relu'))\r\n",
        "    model.add(Dropout(0.5))\r\n",
        "    model.add(Dense(4, activation = 'softmax'))\r\n",
        "\r\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9957 images belonging to 4 classes.\n",
            "Found 9960 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bwqgR7aZep"
      },
      "source": [
        "## Treniranje default modela"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1ntPhZGaZm9",
        "outputId": "d6275bb4-0c60-4a07-fff6-4e2e8a74a3ea"
      },
      "source": [
        "#model_default = model()\r\n",
        "#model_default.fit(dataset_default, steps_per_epoch = None, epochs = 30, verbose = 1)\r\n",
        "#model_default.save('/content/drive/My Drive/Projects/Blood_Cells_Dataset/model_default.h5')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "312/312 [==============================] - 579s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3916 - accuracy: 0.2548\n",
            "Epoch 2/30\n",
            "312/312 [==============================] - 577s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3899 - accuracy: 0.2491\n",
            "Epoch 3/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3876 - accuracy: 0.2534\n",
            "Epoch 4/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3874 - accuracy: 0.2566\n",
            "Epoch 5/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3863 - accuracy: 0.2599\n",
            "Epoch 6/30\n",
            "312/312 [==============================] - 579s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3869 - accuracy: 0.2556\n",
            "Epoch 7/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3866 - accuracy: 0.2558\n",
            "Epoch 8/30\n",
            "312/312 [==============================] - 576s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3860 - accuracy: 0.2571\n",
            "Epoch 9/30\n",
            "312/312 [==============================] - 573s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3866 - accuracy: 0.2579\n",
            "Epoch 10/30\n",
            "312/312 [==============================] - 575s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3861 - accuracy: 0.2581\n",
            "Epoch 11/30\n",
            "312/312 [==============================] - 579s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3860 - accuracy: 0.2621\n",
            "Epoch 12/30\n",
            "312/312 [==============================] - 576s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3853 - accuracy: 0.2667\n",
            "Epoch 13/30\n",
            "312/312 [==============================] - 572s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3854 - accuracy: 0.2573\n",
            "Epoch 14/30\n",
            "312/312 [==============================] - 575s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3853 - accuracy: 0.2704\n",
            "Epoch 15/30\n",
            "312/312 [==============================] - 573s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3849 - accuracy: 0.2666\n",
            "Epoch 16/30\n",
            "312/312 [==============================] - 575s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3854 - accuracy: 0.2592\n",
            "Epoch 17/30\n",
            "312/312 [==============================] - 577s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3852 - accuracy: 0.2668\n",
            "Epoch 18/30\n",
            "312/312 [==============================] - 576s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3853 - accuracy: 0.2630\n",
            "Epoch 19/30\n",
            "312/312 [==============================] - 574s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3843 - accuracy: 0.2711\n",
            "Epoch 20/30\n",
            "312/312 [==============================] - 576s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3847 - accuracy: 0.2612\n",
            "Epoch 21/30\n",
            "312/312 [==============================] - 576s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3841 - accuracy: 0.2666\n",
            "Epoch 22/30\n",
            "312/312 [==============================] - 577s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3835 - accuracy: 0.2688\n",
            "Epoch 23/30\n",
            "312/312 [==============================] - 581s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3831 - accuracy: 0.2750\n",
            "Epoch 24/30\n",
            "312/312 [==============================] - 579s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3818 - accuracy: 0.2808\n",
            "Epoch 25/30\n",
            "312/312 [==============================] - 582s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3815 - accuracy: 0.2852\n",
            "Epoch 26/30\n",
            "312/312 [==============================] - 582s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3816 - accuracy: 0.2797\n",
            "Epoch 27/30\n",
            "312/312 [==============================] - 582s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3808 - accuracy: 0.2883\n",
            "Epoch 28/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3800 - accuracy: 0.2818\n",
            "Epoch 29/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3815 - accuracy: 0.2737\n",
            "Epoch 30/30\n",
            "312/312 [==============================] - 580s 2s/step - batch: 155.5000 - size: 31.9135 - loss: 1.3791 - accuracy: 0.2949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QWH67HaVbw"
      },
      "source": [
        "## Treniranje procesiranog modela"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xshXUciWHDHd",
        "outputId": "1ca93931-6ce0-4774-8661-cdb8675e7451"
      },
      "source": [
        "#model_default = model()\r\n",
        "#model_default.fit(dataset_processed, steps_per_epoch = None, epochs = 30, verbose = 1)\r\n",
        "#model_default.save('/content/drive/My Drive/Projects/Blood_Cells_Dataset/model_processed.h5')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "312/312 [==============================] - 3748s 12s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3834 - accuracy: 0.2537\n",
            "Epoch 2/30\n",
            "312/312 [==============================] - 496s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3760 - accuracy: 0.2758\n",
            "Epoch 3/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3735 - accuracy: 0.2791\n",
            "Epoch 4/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3719 - accuracy: 0.2890\n",
            "Epoch 5/30\n",
            "312/312 [==============================] - 494s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3691 - accuracy: 0.2965\n",
            "Epoch 6/30\n",
            "312/312 [==============================] - 494s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3689 - accuracy: 0.2993\n",
            "Epoch 7/30\n",
            "312/312 [==============================] - 494s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3677 - accuracy: 0.3111\n",
            "Epoch 8/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3663 - accuracy: 0.3096\n",
            "Epoch 9/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3652 - accuracy: 0.3144\n",
            "Epoch 10/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3660 - accuracy: 0.3132\n",
            "Epoch 11/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3631 - accuracy: 0.3208\n",
            "Epoch 12/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3621 - accuracy: 0.3162\n",
            "Epoch 13/30\n",
            "312/312 [==============================] - 494s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3611 - accuracy: 0.3220\n",
            "Epoch 14/30\n",
            "312/312 [==============================] - 494s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3605 - accuracy: 0.3275\n",
            "Epoch 15/30\n",
            "312/312 [==============================] - 493s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3608 - accuracy: 0.3222\n",
            "Epoch 16/30\n",
            "312/312 [==============================] - 495s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3610 - accuracy: 0.3237\n",
            "Epoch 17/30\n",
            "312/312 [==============================] - 496s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3587 - accuracy: 0.3280\n",
            "Epoch 18/30\n",
            "312/312 [==============================] - 496s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3575 - accuracy: 0.3226\n",
            "Epoch 19/30\n",
            "312/312 [==============================] - 500s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3568 - accuracy: 0.3257\n",
            "Epoch 20/30\n",
            "312/312 [==============================] - 505s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3576 - accuracy: 0.3268\n",
            "Epoch 21/30\n",
            "312/312 [==============================] - 503s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3557 - accuracy: 0.3294\n",
            "Epoch 22/30\n",
            "312/312 [==============================] - 499s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3549 - accuracy: 0.3304\n",
            "Epoch 23/30\n",
            "312/312 [==============================] - 498s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3554 - accuracy: 0.3283\n",
            "Epoch 24/30\n",
            "312/312 [==============================] - 497s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3543 - accuracy: 0.3269\n",
            "Epoch 25/30\n",
            "312/312 [==============================] - 497s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3528 - accuracy: 0.3285\n",
            "Epoch 26/30\n",
            "312/312 [==============================] - 497s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3516 - accuracy: 0.3297\n",
            "Epoch 27/30\n",
            "312/312 [==============================] - 501s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3517 - accuracy: 0.3319\n",
            "Epoch 28/30\n",
            "312/312 [==============================] - 500s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3503 - accuracy: 0.3347\n",
            "Epoch 29/30\n",
            "312/312 [==============================] - 498s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3504 - accuracy: 0.3354\n",
            "Epoch 30/30\n",
            "312/312 [==============================] - 498s 2s/step - batch: 155.5000 - size: 31.9231 - loss: 1.3506 - accuracy: 0.3391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6vfH9Cwqven"
      },
      "source": [
        "## Testiranje default modela"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76sURltx0AS"
      },
      "source": [
        "### Testiranje default modela na jednostavnom skupu za testiranje"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C32ctl4RrSXS",
        "outputId": "99b79ec4-be46-4bb3-b4b5-6847683fc9fc"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "import numpy as np\r\n",
        "import glob                         \r\n",
        "\r\n",
        "classes = ['EOSINOPHIL','LYMPHOCYTE','MONOCYTE','NEUTROPHIL']\r\n",
        "model = load_model('/content/drive/My Drive/Projects/Blood_Cells_Dataset/model_processed.h5')\r\n",
        "\r\n",
        "\r\n",
        "def predict_model(cell, dataset):\r\n",
        "\r\n",
        "  cell = cell.lower()\r\n",
        "  cell_count = 0\r\n",
        "  data_counter = 0\r\n",
        "\r\n",
        "  print('\\nProcessing: ' + cell)\r\n",
        "  for path in glob.iglob('/content/drive/My Drive/Projects/Blood_Cells_Dataset/blood_cell_dataset/dataset2-master/dataset2-master/images/' + dataset.upper() + '/' + cell.upper() + '/' + '*.jpeg'):\r\n",
        "      \r\n",
        "      #print(path)\r\n",
        "      img = image.load_img(path, target_size=(80,80))\r\n",
        "      img = image.img_to_array(img)\r\n",
        "\r\n",
        "      img/=255\r\n",
        "      img = img.reshape(1, 80, 80, 3)\r\n",
        "      prediction = model.predict(img)\r\n",
        "      #print(prediction)\r\n",
        "\r\n",
        "      class_name = classes[np.argmax(prediction)]\r\n",
        "      #print(class_name)\r\n",
        "\r\n",
        "      if class_name == cell.upper():\r\n",
        "        cell_count += 1\r\n",
        "      \r\n",
        "      data_counter += 1\r\n",
        "\r\n",
        "  print('Files in '+ dataset.upper() +' folder:', data_counter)\r\n",
        "  print(cell.capitalize() +' count: ' + str(cell_count))\r\n",
        "  print('Correctly classified: ' + str((cell_count/data_counter)*100) + str('%'))\r\n",
        "\r\n",
        "#predict_model('eosinophil', 'TEST_SIMPLE')\r\n",
        "predict_model('lymphocyte', 'TEST_SIMPLE')\r\n",
        "#predict_model('monocyte', 'TEST_SIMPLE')\r\n",
        "#predict_model('neutrophil', 'TEST_SIMPLE')\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: lymphocyte\n",
            "Files in TEST_SIMPLE folder: 6\n",
            "Lymphocyte count: 6\n",
            "Correctly classified: 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}